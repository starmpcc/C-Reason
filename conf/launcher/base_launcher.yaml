name: "???"
torch_dtype: ${dtype:${dtype}}
max_length: 11000 # Total
max_answer_length: 2000 # Answer Only / To prevent repetition
global_batch_size: 35

training_arguments:
  save_strategy: "no"
  output_dir: ${output_dir}
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 32
  learning_rate: 1e-6
  num_train_epochs: 1
  evaluation_strategy: "no"
  tf32: true
  logging_steps: 1
  warmup_ratio: 0.1
  lr_scheduler_type: cosine
  dataloader_num_workers: 4
  gradient_accumulation_steps: ${accumulation_steps:${launcher.training_arguments.per_device_train_batch_size}, ${launcher.global_batch_size}} # Automatically Determined
  optim: "paged_adamw_8bit"
  bf16: ${dtype_flag:${dtype}, bf16}
  fp16: ${dtype_flag:${dtype}, fp16}
  report_to: ${debug_report_to:${debug}}
  gradient_checkpointing: true
  gradient_checkpointing_kwargs: {use_reentrant: False}

llm:
  model: ${model_path}
  tokenizer: ${model_path}
  tokenizer_mode: auto
  trust_remote_code: true
  download_dir: ${model.cache_dir}
  load_format: auto
  dtype: ${dtype}
  seed: 42
  swap_space: 64 # To prevent error in multiple GPU (DataParallel)
  distributed_executor_backend: ray
split: train